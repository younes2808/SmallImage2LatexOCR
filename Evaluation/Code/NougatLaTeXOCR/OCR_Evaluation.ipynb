{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Clone the GitHub repository(Evaluation DATASET)\n",
        "!git clone https://github.com/younes2808/SmallImage2LatexOCR.git\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NEvWWebb5m5m",
        "outputId": "b20b02ac-5c3a-4d20-e327-c768f4e28024"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'SmallImage2LatexOCR'...\n",
            "remote: Enumerating objects: 483, done.\u001b[K\n",
            "remote: Counting objects: 100% (483/483), done.\u001b[K\n",
            "remote: Compressing objects: 100% (471/471), done.\u001b[K\n",
            "remote: Total 483 (delta 18), reused 434 (delta 3), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (483/483), 570.93 KiB | 15.43 MiB/s, done.\n",
            "Resolving deltas: 100% (18/18), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#installing repo\n",
        "https://github.com/NormXU/nougat-latex-ocr.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RzVbls7L0-iG",
        "outputId": "234ecff5-c41c-46f9-8320-91be91f1bab9"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'nougat-latex-ocr'...\n",
            "remote: Enumerating objects: 116, done.\u001b[K\n",
            "remote: Counting objects: 100% (26/26), done.\u001b[K\n",
            "remote: Compressing objects: 100% (12/12), done.\u001b[K\n",
            "remote: Total 116 (delta 19), reused 14 (delta 14), pack-reused 90 (from 1)\u001b[K\n",
            "Receiving objects: 100% (116/116), 120.53 KiB | 10.04 MiB/s, done.\n",
            "Resolving deltas: 100% (32/32), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Entering repo\n",
        "cd nougat-latex-ocr"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ikbQiimA-03S",
        "outputId": "54b51f34-5935-4201-f0de-8466c7841a49"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/nougat-latex-ocr\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import difflib\n",
        "import os\n",
        "import torch\n",
        "from PIL import Image\n",
        "from transformers import VisionEncoderDecoderModel\n",
        "from transformers.models.nougat import NougatTokenizerFast\n",
        "from nougat_latex import NougatLaTexProcessor\n",
        "import time\n",
        "\n",
        "\n",
        "def normalize_latex(latex_string):\n",
        "    \"\"\"\n",
        "    Normalize the LaTeX string by removing unnecessary spaces and ensuring consistent formatting.\n",
        "    \"\"\"\n",
        "    latex_string = latex_string.replace(\" \", \"\").replace(\"\\\\,\", \"\").replace(\"\\\\ \", \"\")\n",
        "    latex_string = latex_string.replace(\"...\", \"\\\\dots\")  # Normalize ellipsis\n",
        "    return latex_string\n",
        "\n",
        "\n",
        "def compare_latex(correct_latex, ocr_latex):\n",
        "    \"\"\"\n",
        "    Compare the correctness of the OCR output with the correct LaTeX expression.\n",
        "    \"\"\"\n",
        "    # Normalize both strings\n",
        "    normalized_correct = normalize_latex(correct_latex)\n",
        "    normalized_ocr = normalize_latex(ocr_latex)\n",
        "\n",
        "    # Use difflib to compare the two strings\n",
        "    diff = difflib.ndiff(normalized_correct, normalized_ocr)\n",
        "    similarity = sum(1 for c in diff if c[0] == \" \") / len(normalized_correct)\n",
        "\n",
        "    return similarity\n",
        "\n",
        "\n",
        "def run_ocr_and_compare(img_path, txt_path, model, tokenizer, latex_processor, device):\n",
        "    \"\"\"\n",
        "    Run OCR on the image and compare the result with the corresponding LaTeX in the text file.\n",
        "    \"\"\"\n",
        "    # Start timer to measure OCR processing time\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Open the image\n",
        "    image = Image.open(img_path)\n",
        "    if not image.mode == \"RGB\":\n",
        "        image = image.convert(\"RGB\")\n",
        "\n",
        "    # Process image for model input\n",
        "    pixel_values = latex_processor(image, return_tensors=\"pt\").pixel_values\n",
        "    decoder_input_ids = tokenizer(\n",
        "        tokenizer.bos_token, add_special_tokens=False, return_tensors=\"pt\"\n",
        "    ).input_ids\n",
        "\n",
        "    # Run OCR\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            pixel_values.to(device),\n",
        "            decoder_input_ids=decoder_input_ids.to(device),\n",
        "            max_length=model.decoder.config.max_length,\n",
        "            early_stopping=True,\n",
        "            pad_token_id=tokenizer.pad_token_id,\n",
        "            eos_token_id=tokenizer.eos_token_id,\n",
        "            use_cache=True,\n",
        "            num_beams=5,\n",
        "            bad_words_ids=[[tokenizer.unk_token_id]],\n",
        "            return_dict_in_generate=True,\n",
        "        )\n",
        "\n",
        "    ocr_latex = tokenizer.batch_decode(outputs.sequences)[0]\n",
        "    ocr_latex = ocr_latex.replace(tokenizer.eos_token, \"\").replace(tokenizer.pad_token, \"\").replace(tokenizer.bos_token, \"\")\n",
        "\n",
        "    # End timer to calculate elapsed time\n",
        "    elapsed_time = time.time() - start_time\n",
        "\n",
        "    # Read the correct LaTeX from the corresponding .txt file\n",
        "    with open(txt_path, \"r\") as file:\n",
        "        correct_latex = file.read().strip()\n",
        "\n",
        "    # Compare the LaTeX strings\n",
        "    similarity_score = compare_latex(correct_latex, ocr_latex)\n",
        "\n",
        "    return similarity_score, ocr_latex, correct_latex, elapsed_time\n",
        "\n",
        "\n",
        "def process_dataset(dataset_dir, model, tokenizer, latex_processor, device, output_file):\n",
        "    \"\"\"\n",
        "    Process the entire dataset, comparing OCR results with ground truth LaTeX and logging results.\n",
        "    \"\"\"\n",
        "    passed_count = 0\n",
        "    total_time = 0\n",
        "    total_comparisons = 0\n",
        "    total_similarity = 0\n",
        "\n",
        "    # Open the output file to write results\n",
        "    with open(output_file, \"w\") as f:\n",
        "        # Loop through all folders from 000 to 100\n",
        "        for i in range(101):  # 0 to 100\n",
        "            folder_name = f\"{str(i).zfill(3)}\"  # Format as 000, 001, ..., 100\n",
        "            folder_path = os.path.join(dataset_dir, folder_name)\n",
        "\n",
        "            if os.path.isdir(folder_path):  # Process only directories\n",
        "                f.write(f\"Processing folder: {folder_name}\\n\")\n",
        "                # Loop through all image files from 000.png to 100.png\n",
        "                for j in range(101):  # 0 to 100\n",
        "                    img_name = f\"{str(j).zfill(3)}.png\"\n",
        "                    txt_name = f\"{str(j).zfill(3)}.txt\"\n",
        "\n",
        "                    img_path = os.path.join(folder_path, img_name)\n",
        "                    txt_path = os.path.join(folder_path, txt_name)\n",
        "\n",
        "                    if os.path.exists(img_path) and os.path.exists(txt_path):\n",
        "                        similarity_score, ocr_latex, correct_latex, elapsed_time = run_ocr_and_compare(\n",
        "                            img_path, txt_path, model, tokenizer, latex_processor, device\n",
        "                        )\n",
        "\n",
        "                        f.write(f\"Folder {folder_name}, Image {img_name}: Similarity = {similarity_score:.4f}\\n\")\n",
        "                        f.write(f\"OCR LaTeX: {ocr_latex}\\n\")\n",
        "                        f.write(f\"Correct LaTeX: {correct_latex}\\n\")\n",
        "                        f.write(f\"Time taken for OCR: {elapsed_time:.4f} seconds\\n\")\n",
        "\n",
        "                        # Provide feedback based on similarity score\n",
        "                        if similarity_score > 0.95:\n",
        "                            f.write(\"OCR output is highly accurate.\\n\")\n",
        "                        elif similarity_score > 0.85:\n",
        "                            f.write(\"OCR output is fairly accurate.\\n\")\n",
        "                        else:\n",
        "                            f.write(\"OCR output has significant differences.\\n\")\n",
        "\n",
        "                        f.write(\"-\" * 50 + \"\\n\")\n",
        "\n",
        "                        # Count the number of \"passed\" results (similarity > 0.9)\n",
        "                        if similarity_score > 0.9:\n",
        "                            passed_count += 1\n",
        "\n",
        "                        # Accumulate total time for calculating average and similarity score\n",
        "                        total_time += elapsed_time\n",
        "                        total_comparisons += 1\n",
        "                        total_similarity += similarity_score\n",
        "\n",
        "        # After processing all files, write the summary to the output file\n",
        "        if total_comparisons > 0:\n",
        "            avg_time = total_time / total_comparisons\n",
        "        else:\n",
        "            avg_time = 0\n",
        "\n",
        "        # Calculating average similarity score\n",
        "        avg_sim_score = total_similarity / total_comparisons\n",
        "\n",
        "        # Writing summary\n",
        "        f.write(\"\\nSummary:\\n\")\n",
        "        f.write(f\"Total number of comparisons: {total_comparisons}\\n\")\n",
        "        f.write(f\"Number of passed comparisons(similarity > 0.9): {passed_count}\\n\")\n",
        "        f.write(f\"Percentage of passed comparisons: {passed_count/total_comparisons:.2%}\\n\")\n",
        "        f.write(f\"Average similarity score: {avg_sim_score:.4f}\\n\")\n",
        "        f.write(f\"Average OCR response time: {avg_time:.4f} seconds\\n\")\n",
        "\n",
        "\n",
        "# Initialize the Nougat model and processor\n",
        "model_name = \"Norm/nougat-latex-base\"\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "model = VisionEncoderDecoderModel.from_pretrained(model_name).to(device)\n",
        "tokenizer = NougatTokenizerFast.from_pretrained(model_name)\n",
        "latex_processor = NougatLaTexProcessor.from_pretrained(model_name)\n",
        "\n",
        "# Define the root directory where your dataset is stored\n",
        "dataset_dir = \"/content/SmallImage2LatexOCR/Dataset\"  # Change this path as needed\n",
        "\n",
        "# Define the output file\n",
        "output_file = \"NougatLaTeXOCR_results.txt\"\n",
        "\n",
        "# Process the entire dataset and write results to the output file\n",
        "process_dataset(dataset_dir, model, tokenizer, latex_processor, device, output_file)\n"
      ],
      "metadata": {
        "id": "Me9KLCfD1CZP"
      },
      "execution_count": 19,
      "outputs": []
    }
  ]
}